# %%
from huggingface_hub import notebook_login
from sentence_transformers import SentenceTransformer, util
from settings import (
    ATLAS_DB,
    ATLAS_DB_COLLECTION,
    ATLAS_VECTOR_INDEX,
    DOCUMENT_EMBEDDINGS_FIELD,
)

MULTI_QA_MINILM_L6_COS_V1 = "multi-qa-MiniLM-L6-cos-v1"
GTE_LARGE = "thenlper/gte-large"
MSMARCO_DISTILROBERTA_BASE_V2 = "sentence-transformers/msmarco-distilroberta-base-v2"

EMBEDDING_MODEL_NAMES = [
    MULTI_QA_MINILM_L6_COS_V1,
    GTE_LARGE,
    MSMARCO_DISTILROBERTA_BASE_V2,
]


class Embedder:
    """A wrapper for a sentence tranformer"""

    def __init__(self, transformer_name: str) -> None:
        """Sentence Transformer with the given transformer_name"""
        self.name = transformer_name
        self.transformer = SentenceTransformer(transformer_name)
        self.token_count = len(self.transformer.encode("hello world"))
        pass

    def encode(self, text: str | list[str]) -> list[float]:
        if not text:
            raise ValueError("Attempted to get embedding for empty text.")

        result = self.transformer.encode(text).tolist()

        return result

    def print_atlas_instructions(self):
        EMBEDDING_LENGTH = len(self.transformer.token_count)
        print(f"embedding length {EMBEDDING_LENGTH}")
        print(f"destination field {DOCUMENT_EMBEDDINGS_FIELD}")
        print(f"*** Create the following vector index in Atlas Search: ***")
        print(f"1. Database '{ATLAS_DB}'")
        print(f"2. Collection '{ATLAS_DB_COLLECTION}'")
        print(f"3. Index name '{ATLAS_VECTOR_INDEX}':")
        print(
            f'4. Index JSON:\n\t{{"fields": [{{"numDimensions": {EMBEDDING_LENGTH},"path": "{DOCUMENT_EMBEDDINGS_FIELD}","similarity": "cosine","type": "vector"}},{{"type": "filter","path": "rating"}}]}}'
        )


def measure_cosine_similarity():
    """Measures cosine similarity of various embedding similarities generated by `encode()`"""
    query = "Should I put cheese on my food?"
    data = [
        "The American approach to everything is Go Big or Go Home!",
        "When it comes to pizza, cover it in cheese completely.",
        "Then add cheese inside the crust, and sprinkle cheese on top.",
    ]

    for m in EMBEDDING_MODEL_NAMES:
        e = Embedder(m)
        print(m, f"({e.token_count} embedding length)")
        v1 = e.encode(query)
        v2 = e.encode(data)

        print(util.cos_sim(v1, v2))


if __name__ == "__main__":
    measure_cosine_similarity()
